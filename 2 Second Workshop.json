{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bd135a-b3bc-4b8a-9d7f-672db9f289fc",
   "metadata": {},
   "source": [
    "# Excercises and Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d55cd-369d-4edb-ade0-70c5bb3c2f4f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f20ac7-2a30-4d67-bbb6-3329e37fb2da",
   "metadata": {},
   "source": [
    "**Task 1**  \n",
    "Goal: Enforce schema for compliance and predictable parsing. Print schemas and peek at data.\n",
    "- [ ] Load `contracts.csv` and `vendors.csv` into DataFrames with schema enforced.\n",
    "- [ ] Print schemas\n",
    "- [ ] Print sample 5 rows\n",
    "- [ ] Print the rows count of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddcbbe-f8d5-4263-8111-88e538d93233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "token = \"your_token\"\n",
    "user  = \"your_user\"\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .remote(\n",
    "        f\"sc://training.iomete.com:443/;cluster=dell-training;data_plane=spark-resources-1;use_ssl=true;user_id={user};api_token={token}\"\n",
    "    )\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ce091-0da6-4c3f-9285-3a814f6620c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    ")\n",
    "\n",
    "# Explicit Spark schemas (enforce types for compliance)\n",
    "contracts_schema = StructType([\n",
    "    StructField(\"contract_id\", StringType(), False),\n",
    "    StructField(\"agency_name\", StringType(), True),\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"vendor_name\", StringType(), True),\n",
    "    StructField(\"obligation_amount\", DoubleType(), True),\n",
    "    StructField(\"fiscal_year\", IntegerType(), True),\n",
    "    StructField(\"contract_date\", StringType(), True),  # read as string then cast to date\n",
    "])\n",
    "\n",
    "vendors_schema = StructType([\n",
    "    StructField(\"vendor_id\", StringType(), True),\n",
    "    StructField(\"vendor_name\", StringType(), True),\n",
    "    StructField(\"vendor_category\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba4893e-2e23-4b1e-88c8-b76f0645120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRACTS_CSV_LOCATION = \"s3a://lakehouse/datasets/contracts.csv\"\n",
    "VENDORS_CSV_LOCATION   = \"s3a://lakehouse/datasets/vendors.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d532aed6-e77f-42fe-a418-dcb108159f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, col\n",
    "\n",
    "contracts = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .schema(contracts_schema)\n",
    "    .csv(CONTRACTS_CSV_LOCATION)\n",
    ")\n",
    "\n",
    "vendor = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .schema(vendors_schema)\n",
    "    .csv(VENDORS_CSV_LOCATION)\n",
    ")\n",
    "\n",
    "contracts_with_date = contracts.withColumn('contract_date', to_date(col(\"contract_date\"), 'yyyy-MM-dd'))\n",
    "\n",
    "print(\"Contracts table's row count:\", contracts.count())\n",
    "print(\"Vendors table's row count:\", vendor.count())\n",
    "\n",
    "print(contracts_with_date.printSchema())\n",
    "contracts_with_date.show(1)\n",
    "vendor.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c45d0-4185-4ddc-98f3-0c7950edd47f",
   "metadata": {},
   "source": [
    "## Task 2: Clean nulls and standardize text\n",
    "**Goal:** Improve join quality and downstream aggregations.\n",
    "\n",
    "- Trim whitespace on `agency_name` and `vendor_name`\n",
    "- Uppercase `agency_name`\n",
    "- Replace null/empty `vendor_name` with `'UNKNOWN_VENDOR'`\n",
    "- Optionally drop rows with critical nulls (e.g., `obligation_amount`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5cfda-821e-4518-a346-0d6b2bd86d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import trim, upper, concat, lit, initcap, lower\n",
    "\n",
    "def std(column):\n",
    "    return initcap(lower(trim(col(column))))\n",
    "                   \n",
    "contracts_clean  = (\n",
    "    contracts_with_date\n",
    "    .withColumn(\"agency_name\", std(\"agency_name\"))\n",
    "    .withColumn(\"vendor_name\", std(\"vendor_name\"))\n",
    "    .withColumn(\"obligation_amount\", col(\"obligation_amount\").cast(\"double\"))   \n",
    "    .withColumn(\"obligation_amount_dollar\", concat(lit('$'), col(\"obligation_amount\")))\n",
    ")\n",
    "\n",
    "contracts_clean.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e514de-d157-4dbc-a735-4715a1e3f3d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c106b5c-53db-47bb-93a5-dff3b5a913e8",
   "metadata": {},
   "source": [
    "# Writing into Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69c83a-18c5-4860-8ea7-c8d95dc837d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_NAME = f\"spark_catalog.workshop_db.{user}_contracts_clean\"\n",
    "\n",
    "(\n",
    "    contracts_clean\n",
    "    .writeTo(TABLE_NAME)\n",
    "    .using(\"iceberg\")\n",
    "    .partitionedBy(contracts_clean.fiscal_year)\n",
    "    .createOrReplace()\n",
    ")\n",
    "\n",
    "df_iceberg = spark.table(TABLE_NAME)\n",
    "df_iceberg.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc3a58-be48-45ab-928c-83494a337733",
   "metadata": {},
   "source": [
    "# Temp Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6be90d5e-a35c-4850-a91e-0a4057435bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    contracts_clean\n",
    "    .select(\"contract_id\", \"vendor_id\", \"fiscal_year\")\n",
    "    .filter(contracts_clean.fiscal_year == 2025)\n",
    "    .createOrReplaceTempView('minified_contracts')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095dfe2-7839-4875-abc4-ddcf79d6c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select count(1) from minified_contracts group by fiscal_year').createOrReplaceTempView('agg_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bef672-ffd5-4367-a1c9-449f09d58aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('select * from agg_data').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece26db4-e280-4330-abe4-502ad83ee3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_df = (contracts.groupBy(\"vendor_name\")\n",
    "            .agg(F.sum(\"obligation_amount\").alias(\"total_spend\"))\n",
    "            .orderBy(F.desc(\"total_spend\"))\n",
    "            .limit(10))\n",
    "\n",
    "contracts.createOrReplaceTempView(\"contracts\")\n",
    "top10_sql = spark.sql(\"\"\"\n",
    "    SELECT vendor_name, SUM(obligation_amount) AS total_spend\n",
    "    FROM contracts\n",
    "    GROUP BY vendor_name\n",
    "    ORDER BY total_spend DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"DataFrame API:\"); top10_df.show(truncate=False)\n",
    "print(\"SQL:\"); top10_sql.show(truncate=False)\n",
    "\n",
    "diff = top10_df.subtract(top10_sql).unionByName(top10_sql.subtract(top10_df))\n",
    "print(\"Differences (should be 0):\", diff.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2a3ab-805b-4ce5-ad21-e79b2a6b832b",
   "metadata": {},
   "source": [
    "# JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0e1c08eb-20e8-4c5e-bf38-e60ac17b646c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-----------------+-----------+-------------+---------------+---------+\n",
      "|vendor_id|contract_id|vendor_name|obligation_amount|fiscal_year|contract_date|vendor_category|  country|\n",
      "+---------+-----------+-----------+-----------------+-----------+-------------+---------------+---------+\n",
      "|   V00136|   C0000001| Vendor_136|          10000.0|       2021|   2021-03-17|      Aerospace|      USA|\n",
      "|   V00425|   C0000002| Vendor_425|          10000.0|       2024|   2024-08-10|     Healthcare|       UK|\n",
      "|   V00805|   C0000003| Vendor_805|          10000.0|       2024|   2024-07-25|      Aerospace|      USA|\n",
      "|   V00921|   C0000004| Vendor_921|          10000.0|       2022|   2022-08-09|      Aerospace|Australia|\n",
      "+---------+-----------+-----------+-----------------+-----------+-------------+---------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contracts_new = spark.read.csv(CONTRACTS_CSV_LOCATION, header=True, inferSchema=True)\n",
    "vendors_new = spark.read.csv(VENDORS_CSV_LOCATION, header=True, inferSchema=True)\n",
    "\n",
    "contracts_new.join(vendors_new, 'vendor_id').drop(vendors_new.vendor_name).drop(contracts_new.agency_name).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acf9c38f-ef32-4107-a9b8-19f6ed2ffdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-----------+-----------------+-----------+-------------+---------------+---------+\n",
      "|vendor_id|contract_id|vendor_name|obligation_amount|fiscal_year|contract_date|vendor_category|  country|\n",
      "+---------+-----------+-----------+-----------------+-----------+-------------+---------------+---------+\n",
      "|   V00136|   C0000001| Vendor_136|          10000.0|       2021|   2021-03-17|      Aerospace|      USA|\n",
      "|   V00425|   C0000002| Vendor_425|          10000.0|       2024|   2024-08-10|     Healthcare|       UK|\n",
      "|   V00805|   C0000003| Vendor_805|          10000.0|       2024|   2024-07-25|      Aerospace|      USA|\n",
      "|   V00921|   C0000004| Vendor_921|          10000.0|       2022|   2022-08-09|      Aerospace|Australia|\n",
      "+---------+-----------+-----------+-----------------+-----------+-------------+---------------+---------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "contracts_new.join(broadcast(vendors_new), 'vendor_id').drop(vendors_new.vendor_name).drop(contracts_new.agency_name).show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8826b4cb-e608-4c1e-ba45-73c8cbba5015",
   "metadata": {},
   "source": [
    "## Shuffle Join\n",
    "- Both datasets are partitioned by join keys across all executors (nodes)\n",
    "- Data with the same join keys gets moved to the same partitions (expensive network shuffle)\n",
    "- Good for large-to-large table (dataset) joins - but involves network overhead\n",
    "\n",
    "## Broadcast Join\n",
    "- Small datasets (vendors = 2K rows) is copied to all the executors (nodes) in memory\n",
    "- Eliminate expensive shuffle operations, since join happens locally on each executor\n",
    "- Much faster for large-to-small dataset (< 10 MB by default)\n",
    "- Trade-Off: uses more memory but saves significant network I/O operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694c2806-64bf-40e1-a162-8ab92e278172",
   "metadata": {},
   "source": [
    "# Partitioning & Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c56937f2-022f-433d-9d50-e1da07976232",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_new \\\n",
    ".write \\\n",
    ".partitionBy('fiscal_year') \\\n",
    ".parquet('s3a://lakehouse/output-data/fuad_contracts_new_partitioned', mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28e57c-16ab-4b61-8307-5b0beeef5805",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_data = spark.read.parquet('s3a://lakehouse/output-data/fuad_contracts_new_partitioned')\n",
    "parquet_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "05394c2c-fc8b-4cc7-932c-20d930d52a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_new \\\n",
    ".write \\\n",
    ".bucketBy(5, 'vendor_name') \\\n",
    ".saveAsTable('workshop_db.fuad_contracts_new_bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdec9de-2565-43bc-a90c-8a1a33a53c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('DESC EXTENDED workshop_db.fuad_contracts_new_bucket').show(truncate=False)\n",
    "\n",
    "df_bucketed_contracts = spark.table('workshop_db.fuad_contracts_new_bucket')\n",
    "df_bucketed_contracts.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d079d3-a050-44e8-9bf4-a093e24c59cd",
   "metadata": {},
   "source": [
    "## Buckets\n",
    "- Creates 5 (how much is specified) folders(buckets) on vendor_name column's hash\n",
    "- Eliminates shuffle during joins/agg on bucketed column\n",
    "\n",
    "## Partitions\n",
    "- Creates separate folders for each fiscal_year (e.g., fiscal_year=2020/, fiscal_year=2021/)\n",
    "- Reduces I/O and improves query performance for filtered queries: partitioned_contracts.filter(\"fiscal_year = 2020\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
